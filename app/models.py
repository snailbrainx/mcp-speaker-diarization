from sqlalchemy import Column, Integer, String, Float, DateTime, ForeignKey, LargeBinary, Text, Boolean, UniqueConstraint
from sqlalchemy.orm import relationship
from datetime import datetime
from .database import Base
import json

class Speaker(Base):
    __tablename__ = "speakers"

    id = Column(Integer, primary_key=True, index=True)
    name = Column(String, unique=True, index=True, nullable=False)
    embedding = Column(LargeBinary, nullable=False)  # Stored as numpy array bytes
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

    # Per-speaker emotion matching threshold (NULL = use global default)
    emotion_threshold = Column(Float, nullable=True)

    # Relationships
    segments = relationship("Segment", back_populates="speaker")
    emotion_profiles = relationship("SpeakerEmotionProfile", back_populates="speaker", cascade="all, delete-orphan")

    def get_embedding(self):
        """Convert binary embedding back to numpy array"""
        import numpy as np
        return np.frombuffer(self.embedding, dtype=np.float32)

    def set_embedding(self, embedding_array):
        """Convert numpy array to binary for storage"""
        import numpy as np
        self.embedding = embedding_array.astype(np.float32).tobytes()


class SpeakerEmotionProfile(Base):
    """
    Emotion embeddings for a specific speaker and emotion category.
    Stores learned emotional signatures that improve with corrections.
    """
    __tablename__ = "speaker_emotion_profiles"

    id = Column(Integer, primary_key=True, index=True)
    speaker_id = Column(Integer, ForeignKey("speakers.id", ondelete="CASCADE"), nullable=False)
    emotion_category = Column(String, nullable=False)  # 'angry', 'happy', 'sad', etc.

    # Emotion embedding (1024-D from emotion2vec)
    embedding = Column(LargeBinary, nullable=False)  # Numpy array -> bytes

    # Metadata
    sample_count = Column(Integer, default=1)  # How many corrections went into this
    confidence_threshold = Column(Float, nullable=True)  # Per-speaker-per-emotion threshold (NULL = use speaker/global)

    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

    # Unique constraint: one profile per speaker per emotion
    __table_args__ = (
        UniqueConstraint('speaker_id', 'emotion_category', name='_speaker_emotion_uc'),
    )

    # Relationships
    speaker = relationship("Speaker", back_populates="emotion_profiles")

    def get_embedding(self):
        """Convert binary embedding to numpy array"""
        import numpy as np
        return np.frombuffer(self.embedding, dtype=np.float32)

    def set_embedding(self, embedding_array):
        """Convert numpy array to binary"""
        import numpy as np
        self.embedding = embedding_array.astype(np.float32).tobytes()


class Recording(Base):
    __tablename__ = "recordings"

    id = Column(Integer, primary_key=True, index=True)
    filename = Column(String, nullable=False)
    duration = Column(Float)  # Duration in seconds
    processed_at = Column(DateTime, default=datetime.utcnow)
    status = Column(String, default="processing")  # processing, completed, failed

    # Relationship to segments
    segments = relationship("Segment", back_populates="recording", cascade="all, delete-orphan")


class Segment(Base):
    __tablename__ = "segments"

    id = Column(Integer, primary_key=True, index=True)
    recording_id = Column(Integer, ForeignKey("recordings.id"), nullable=False)
    speaker_id = Column(Integer, ForeignKey("speakers.id"), nullable=True)  # Null if unknown
    start_time = Column(Float, nullable=False)  # Start time in seconds
    end_time = Column(Float, nullable=False)  # End time in seconds
    confidence = Column(Float)  # Confidence score for speaker match
    speaker_label = Column(String)  # e.g., "SPEAKER_00", "Unknown_01", or actual name

    # Relationships
    recording = relationship("Recording", back_populates="segments")
    speaker = relationship("Speaker", back_populates="segments")


class Conversation(Base):
    """
    Represents a continuous recording session (e.g., a meeting, interview, etc.)
    """
    __tablename__ = "conversations"

    id = Column(Integer, primary_key=True, index=True)
    title = Column(String, nullable=True)  # Auto-generated or user-set
    start_time = Column(DateTime, nullable=False, default=datetime.utcnow)
    end_time = Column(DateTime, nullable=True)  # Null while recording
    duration = Column(Float, nullable=True)  # Duration in seconds
    status = Column(String, default="recording")  # recording, processing, completed, failed
    audio_path = Column(String, nullable=True)  # Path to WAV or MP3 file
    audio_format = Column(String, default="wav")  # wav or mp3
    num_segments = Column(Integer, default=0)
    num_speakers = Column(Integer, default=0)
    created_at = Column(DateTime, default=datetime.utcnow)

    # Relationships
    transcript_segments = relationship("ConversationSegment", back_populates="conversation", cascade="all, delete-orphan")


class ConversationSegment(Base):
    """
    Individual speech segment within a conversation with transcription
    """
    __tablename__ = "conversation_segments"

    id = Column(Integer, primary_key=True, index=True)
    conversation_id = Column(Integer, ForeignKey("conversations.id"), nullable=False)
    speaker_id = Column(Integer, ForeignKey("speakers.id", ondelete="SET NULL"), nullable=True)  # Null for unknown - auto-set to NULL when speaker deleted
    speaker_name = Column(String, nullable=True)  # Denormalized for quick access
    text = Column(Text, nullable=True)  # Transcription text

    # Absolute timestamps (for AI context)
    start_time = Column(DateTime, nullable=False)
    end_time = Column(DateTime, nullable=False)

    # Relative timestamps (seconds from conversation start, for audio playback)
    start_offset = Column(Float, nullable=False)
    end_offset = Column(Float, nullable=False)

    # Individual segment audio file (for streaming playback before concatenation)
    segment_audio_path = Column(String, nullable=True)

    confidence = Column(Float, nullable=True)  # Speaker identification confidence

    # Emotion detection (from emotion2vec)
    emotion_category = Column(String, nullable=True)  # Primary emotion label (happy, sad, angry, etc.)
    emotion_confidence = Column(Float, nullable=True)  # Confidence score for emotion prediction

    # Emotion correction tracking (for personalized learning)
    emotion_corrected = Column(Boolean, default=False, nullable=False)  # True if user manually corrected
    emotion_corrected_at = Column(DateTime, nullable=True)  # When correction was made
    emotion_misidentified = Column(Boolean, default=False, nullable=False)  # True if emotion correction was wrong (exclude from profile)

    # Word-level transcription data with confidence scores (JSON)
    words_data = Column(Text, nullable=True)  # Stores JSON array of {word, start, end, probability}
    avg_logprob = Column(Float, nullable=True)  # Segment-level average log probability

    processed_at = Column(DateTime, default=datetime.utcnow)

    # Misidentification tracking
    is_misidentified = Column(Boolean, default=False, nullable=False)  # True if this segment was wrongly assigned to current speaker

    # Relationships
    conversation = relationship("Conversation", back_populates="transcript_segments")
    speaker = relationship("Speaker")

    @property
    def words(self):
        """Parse words_data JSON and return as list"""
        if self.words_data:
            try:
                import json
                return json.loads(self.words_data)
            except:
                return None
        return None


class GroundTruthLabel(Base):
    """
    Ground truth speaker labels for testing and optimization
    Stores manual identifications WITHOUT affecting the actual segments
    """
    __tablename__ = "ground_truth_labels"

    id = Column(Integer, primary_key=True, index=True)
    segment_id = Column(Integer, ForeignKey("conversation_segments.id"), nullable=False)
    true_speaker_name = Column(String, nullable=False)  # Manual identification
    labeled_by = Column(String, default="user")  # Who labeled it
    labeled_at = Column(DateTime, default=datetime.utcnow)
    notes = Column(Text, nullable=True)  # Optional notes about the segment

    # Relationship
    segment = relationship("ConversationSegment")
